{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHL73qSe35sU"
   },
   "source": [
    "# PapyrusVision Hieroglyph Detection - Model Training\n",
    "\n",
    "This notebook trains a Detectron2 model for hieroglyph detection and classification.\n",
    "\n",
    "## Objectives:\n",
    "1. Set up Detectron2 configuration for hieroglyph detection\n",
    "2. Configure training parameters and data augmentation\n",
    "3. Train the model with monitoring and logging\n",
    "4. Visualize training progress\n",
    "5. Save the trained model\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 19490,
     "status": "ok",
     "timestamp": 1754593201381,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "lotiHVG335sW",
    "outputId": "fdce31be-7b97-4595-8b46-1abdeecb2dd3"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -U torch torchvision cython\n",
    "!pip install -U \"git+https://github.com/facebookresearch/fvcore.git\" \"git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\"\n",
    "import torch, torchvision\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12014,
     "status": "ok",
     "timestamp": 1754593213497,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "KD7uRM-g35sW",
    "outputId": "2bab13dc-917d-43d5-862f-cfe09cea3c98"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/facebookresearch/detectron2 detectron2_repo\n",
    "!pip install -e detectron2_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2762,
     "status": "ok",
     "timestamp": 1754593216262,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "br_WjMQs35sX",
    "outputId": "bf611287-0811-499b-fe28-f17cf94581a6"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Check CUDA version to pick the correct Detectron2 wheel\n",
    "cuda_version = torch.version.cuda\n",
    "print(f\"Detected CUDA version: {cuda_version}\")\n",
    "\n",
    "# Install other required packages\n",
    "print(\"\\nInstalling other required packages...\")\n",
    "other_packages = [\n",
    "    \"opencv-python-headless\",\n",
    "    \"matplotlib>=3.3.0\",\n",
    "    \"seaborn>=0.11.0\",\n",
    "    \"plotly>=5.0.0\",\n",
    "    \"pandas>=1.3.0\",\n",
    "    \"scikit-learn>=1.0.0\",\n",
    "    \"kaleido>=0.2.1\",\n",
    "    \"pycocotools\",\n",
    "    \"pillow>=8.0.0\",\n",
    "    \"numpy\",\n",
    "    \"tqdm\",\n",
    "    \"ipywidgets\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + other_packages)\n",
    "    print(\"All other packages installed successfully!\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Failed to install other packages: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Installation of other packages failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23847,
     "status": "ok",
     "timestamp": 1754593240115,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "UmtFfvVi35sX",
    "outputId": "55023878-8a57-4fde-eaf8-9ca623c5b382"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# Detectron2 imports\n",
    "import detectron2\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor, DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_train_loader, build_detection_test_loader\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.solver import build_lr_scheduler, build_optimizer\n",
    "from detectron2.utils.events import EventStorage\n",
    "import detectron2.data.transforms as T\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Set up paths\n",
    "PROJECT_ROOT = \"/content/drive/My Drive/PapyrusNU_Detectron\"\n",
    "DATA_DIR = f\"{PROJECT_ROOT}/data\"\n",
    "SCRIPTS_DIR = f\"{PROJECT_ROOT}/scripts\"\n",
    "MODELS_DIR = f\"{PROJECT_ROOT}/models\"\n",
    "NOTEBOOKS_DIR = f\"{PROJECT_ROOT}/notebooks\"\n",
    "\n",
    "# Add scripts to path\n",
    "sys.path.append(SCRIPTS_DIR)\n",
    "\n",
    "# Set up logging\n",
    "setup_logger()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\"Detectron2 version: {detectron2.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojf8c-Cg35sX"
   },
   "source": [
    "## Data Loading and Category Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2568,
     "status": "ok",
     "timestamp": 1754593245626,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "Zep-3c1j35sX",
    "outputId": "eb509a84-3bb8-4538-b5af-ee86e6797c83"
   },
   "outputs": [],
   "source": [
    "# Load split datasets and create master category mapping\n",
    "with open(f\"{DATA_DIR}/annotations/train_annotations.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "with open(f\"{DATA_DIR}/annotations/val_annotations.json\", \"r\") as f:\n",
    "    val_data = json.load(f)\n",
    "with open(f\"{DATA_DIR}/annotations/test_annotations.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded annotation files:\")\n",
    "print(f\"Train: {len(train_data[\"annotations\"])} annotations, {len(train_data[\"categories\"])} categories\")\n",
    "print(f\"Val: {len(val_data[\"annotations\"])} annotations, {len(val_data[\"categories\"])} categories\")\n",
    "print(f\"Test: {len(test_data[\"annotations\"])} annotations, {len(test_data[\"categories\"])} categories\")\n",
    "\n",
    "# Create a master category list from all splits\n",
    "master_categories = {}\n",
    "for data in [train_data, val_data, test_data]:\n",
    "    for cat in data.get(\"categories\", []):\n",
    "        master_categories[cat[\"id\"]] = cat\n",
    "\n",
    "# Sort categories and create a mapping from original COCO IDs to 0-indexed IDs for Detectron2\n",
    "sorted_cat_ids = sorted(master_categories.keys())\n",
    "category_id_map = {old_id: new_id for new_id, old_id in enumerate(sorted_cat_ids)}\n",
    "category_names = [master_categories[old_id][\"name\"] for old_id in sorted_cat_ids]\n",
    "\n",
    "print(f\"\\n Created master category mapping:\")\n",
    "print(f\"Total unique categories: {len(category_names)}\")\n",
    "print(f\"Original ID range: {min(sorted_cat_ids)} - {max(sorted_cat_ids)}\")\n",
    "print(f\"New ID range: 0 - {len(category_names)-1}\")\n",
    "print(f\"Sample categories: {category_names[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 90,
     "status": "ok",
     "timestamp": 1754593248703,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "8btLxY8-35sY",
    "outputId": "b84ea164-b621-4a74-db8a-bbf249e14009"
   },
   "outputs": [],
   "source": [
    "# Function to load data and remap category IDs for Detectron2\n",
    "def load_and_remap_dataset(data, image_dir, id_map):\n",
    "    \"\"\"Load COCO format data and remap category IDs to be 0-indexed\"\"\"\n",
    "    dataset_dicts = []\n",
    "    images = {img[\"id\"]: img for img in data[\"images\"]}\n",
    "\n",
    "    # Group annotations by image\n",
    "    annotations_by_image = {}\n",
    "    for ann in data[\"annotations\"]:\n",
    "        img_id = ann[\"image_id\"]\n",
    "        if img_id not in annotations_by_image:\n",
    "            annotations_by_image[img_id] = []\n",
    "        annotations_by_image[img_id].append(ann)\n",
    "\n",
    "    for image_id, image_info in images.items():\n",
    "        record = {}\n",
    "        record[\"file_name\"] = os.path.join(image_dir, image_info[\"file_name\"])\n",
    "        record[\"height\"] = image_info[\"height\"]\n",
    "        record[\"width\"] = image_info[\"width\"]\n",
    "        record[\"image_id\"] = image_id\n",
    "\n",
    "        objs = []\n",
    "        for ann in annotations_by_image.get(image_id, []):\n",
    "            # Apply the remapping to the category ID\n",
    "            remapped_id = id_map.get(ann[\"category_id\"])\n",
    "            if remapped_id is not None:\n",
    "                obj = {\n",
    "                    \"bbox\": ann[\"bbox\"],\n",
    "                    \"bbox_mode\": 1,\n",
    "                    \"segmentation\": ann.get(\"segmentation\", []),\n",
    "                    \"category_id\": remapped_id,\n",
    "                    \"iscrowd\": ann.get(\"iscrowd\", 0),\n",
    "                }\n",
    "                objs.append(obj)\n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts\n",
    "\n",
    "# Load and remap all data splits\n",
    "image_dir = f\"{DATA_DIR}/images\"\n",
    "train_dataset_dicts = load_and_remap_dataset(train_data, image_dir, category_id_map)\n",
    "val_dataset_dicts = load_and_remap_dataset(val_data, image_dir, category_id_map)\n",
    "test_dataset_dicts = load_and_remap_dataset(test_data, image_dir, category_id_map)\n",
    "\n",
    "print(f\"\\n Loaded and remapped datasets:\")\n",
    "print(f\"Train: {len(train_dataset_dicts)} images\")\n",
    "print(f\"Val: {len(val_dataset_dicts)} images\")\n",
    "print(f\"Test: {len(test_dataset_dicts)} images\")\n",
    "\n",
    "# Verify remapping worked correctly\n",
    "all_cat_ids = set()\n",
    "for dataset_dicts in [train_dataset_dicts, val_dataset_dicts, test_dataset_dicts]:\n",
    "    for record in dataset_dicts:\n",
    "        for ann in record[\"annotations\"]:\n",
    "            all_cat_ids.add(ann[\"category_id\"])\n",
    "\n",
    "print(f\"Remapped category IDs range: {min(all_cat_ids)} - {max(all_cat_ids)}\")\n",
    "print(f\"Expected range: 0 - {len(category_names)-1}\")\n",
    "assert max(all_cat_ids) < len(category_names), \"Category ID mapping failed!\"\n",
    "print(\"Category ID remapping successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6q2voP5p35sY"
   },
   "source": [
    "## Dataset Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74,
     "status": "ok",
     "timestamp": 1754593252156,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "o8X4IcoT35sY",
    "outputId": "fb2cb523-23b0-4b61-e6cb-74f2663b1d39"
   },
   "outputs": [],
   "source": [
    "# Register datasets with Detectron2\n",
    "dataset_names = [\"hieroglyphs_train\", \"hieroglyphs_val\", \"hieroglyphs_test\"]\n",
    "dataset_dicts_list = [train_dataset_dicts, val_dataset_dicts, test_dataset_dicts]\n",
    "\n",
    "# Clear existing registrations\n",
    "for name in dataset_names:\n",
    "    if name in DatasetCatalog.list():\n",
    "        DatasetCatalog.remove(name)\n",
    "\n",
    "# Register datasets\n",
    "for name, dataset_dicts in zip(dataset_names, dataset_dicts_list):\n",
    "    DatasetCatalog.register(name, lambda d=dataset_dicts: d)\n",
    "    MetadataCatalog.get(name).set(\n",
    "        thing_classes=category_names,\n",
    "        evaluator_type=\"coco\"\n",
    "    )\n",
    "\n",
    "print(f\"Datasets registered successfully!\")\n",
    "print(f\"Number of categories: {len(category_names)}\")\n",
    "print(f\"Sample categories: {category_names[:5]}...\")\n",
    "\n",
    "# Verify registration\n",
    "hieroglyphs_metadata = MetadataCatalog.get(\"hieroglyphs_train\")\n",
    "print(f\"Metadata classes: {len(hieroglyphs_metadata.thing_classes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2l_XMvz35sY"
   },
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 76,
     "status": "ok",
     "timestamp": 1754593254905,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "MCzGcomf35sZ",
    "outputId": "9858b8c3-2b8f-42c8-a5ae-2a24a4e5653e"
   },
   "outputs": [],
   "source": [
    "# Set up model configuration\n",
    "cfg = get_cfg()\n",
    "\n",
    "# Model architecture - using Mask R-CNN with ResNet-50 backbone\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "\n",
    "# Dataset configuration\n",
    "cfg.DATASETS.TRAIN = (\"hieroglyphs_train\",)\n",
    "cfg.DATASETS.TEST = (\"hieroglyphs_val\",)\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "\n",
    "# Model parameters\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(category_names)  # Number of hieroglyph categories\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128  # Reduced due to small object sizes\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Detection threshold\n",
    "\n",
    "# Training configuration\n",
    "cfg.SOLVER.IMS_PER_BATCH = 1  # Batch size (limited by single image dataset)\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # Learning rate\n",
    "cfg.SOLVER.MAX_ITER = 5000  # Number of training iterations\n",
    "cfg.SOLVER.STEPS = (3000, 4500)  # Learning rate decay steps\n",
    "cfg.SOLVER.GAMMA = 0.1  # Learning rate decay factor\n",
    "cfg.SOLVER.WARMUP_ITERS = 500  # Warmup iterations\n",
    "cfg.SOLVER.WARMUP_FACTOR = 1.0 / 1000\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 1000  # Save checkpoint every 1000 iterations\n",
    "\n",
    "# Evaluation configuration\n",
    "cfg.TEST.EVAL_PERIOD = 1000  # Evaluate every 1000 iterations\n",
    "\n",
    "# Output directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"{MODELS_DIR}/hieroglyph_model_{timestamp}\"\n",
    "cfg.OUTPUT_DIR = output_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Model Configuration:\")\n",
    "print(f\"Architecture: Mask R-CNN with ResNet-50 FPN\")\n",
    "print(f\"Number of classes: {cfg.MODEL.ROI_HEADS.NUM_CLASSES}\")\n",
    "print(f\"Batch size: {cfg.SOLVER.IMS_PER_BATCH}\")\n",
    "print(f\"Learning rate: {cfg.SOLVER.BASE_LR}\")\n",
    "print(f\"Max iterations: {cfg.SOLVER.MAX_ITER}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Save configuration\n",
    "with open(f\"{output_dir}/config.yaml\", \"w\") as f:\n",
    "    f.write(cfg.dump())\n",
    "print(f\"Configuration saved to {output_dir}/config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGAoESGf35sZ"
   },
   "source": [
    "## Data Augmentation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 96,
     "status": "ok",
     "timestamp": 1754593297368,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "e-6_qBaI35sZ",
    "outputId": "4d91740e-9e9d-46e1-c98f-94a642de0f5b"
   },
   "outputs": [],
   "source": [
    "# Custom data augmentation for hieroglyphs\n",
    "from detectron2.data import detection_utils as utils\n",
    "import copy # Import the copy module\n",
    "\n",
    "def custom_mapper(dataset_dict):\n",
    "    \"\"\"\n",
    "    Custom data mapper with augmentations suitable for hieroglyphs\n",
    "    \"\"\"\n",
    "    dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
    "\n",
    "    # Load image\n",
    "    image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n",
    "\n",
    "    # Define augmentations\n",
    "    augmentations = [\n",
    "        T.RandomFlip(prob=0.5, horizontal=True),  # Horizontal flip\n",
    "        T.RandomBrightness(0.8, 1.2),  # Brightness adjustment\n",
    "        T.RandomContrast(0.8, 1.2),  # Contrast adjustment\n",
    "        T.RandomSaturation(0.8, 1.2),  # Saturation adjustment\n",
    "        T.RandomRotation(angle=[-5, 5]),  # Small rotations\n",
    "        T.ResizeShortestEdge(\n",
    "            short_edge_length=[640, 672, 704, 736, 768, 800],\n",
    "            max_size=1333,\n",
    "            sample_style=\"choice\"\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Apply augmentations\n",
    "    aug_input = T.AugInput(image)\n",
    "    transforms = T.AugmentationList(augmentations)(aug_input)\n",
    "    image = aug_input.image\n",
    "\n",
    "    # Transform annotations\n",
    "    annos = [\n",
    "        utils.transform_instance_annotations(obj, transforms, image.shape[:2])\n",
    "        for obj in dataset_dict.pop(\"annotations\")\n",
    "        if obj.get(\"iscrowd\", 0) == 0\n",
    "    ]\n",
    "\n",
    "    instances = utils.annotations_to_instances(annos, image.shape[:2])\n",
    "    dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
    "    dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n",
    "\n",
    "    return dataset_dict\n",
    "\n",
    "print(\"Custom data mapper with augmentations defined:\")\n",
    "print(\"- Random horizontal flips\")\n",
    "print(\"- Brightness, contrast, saturation adjustments\")\n",
    "print(\"- Small random rotations (Â±5 degrees)\")\n",
    "print(\"- Multi-scale training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gjR0smz35sZ"
   },
   "source": [
    "## Custom Trainer with Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 91,
     "status": "ok",
     "timestamp": 1754593261638,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "gapQXXkj35sZ",
    "outputId": "922dfba6-951f-495a-e35f-e35976eea969"
   },
   "outputs": [],
   "source": [
    "# Custom trainer class with enhanced monitoring\n",
    "class HieroglyphTrainer(DefaultTrainer):\n",
    "    \"\"\"\n",
    "    Custom trainer for hieroglyph detection with enhanced logging and evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "        self.training_history = {\n",
    "            \"iterations\": [],\n",
    "            \"total_loss\": [],\n",
    "            \"learning_rate\": [],\n",
    "            \"validation_ap\": [],\n",
    "            \"validation_iterations\": []\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        \"\"\"Build training data loader with custom augmentations\"\"\"\n",
    "        return build_detection_train_loader(cfg, mapper=custom_mapper)\n",
    "\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        \"\"\"Build evaluator for validation\"\"\"\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        return COCOEvaluator(dataset_name, cfg, False, output_folder)\n",
    "\n",
    "    def run_step(self):\n",
    "        \"\"\"Custom run step with loss logging\"\"\"\n",
    "        assert self.model.training, \"[Trainer] model was changed to eval mode!\"\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        data = next(self._trainer._data_loader_iter)\n",
    "        data_time = time.perf_counter() - start\n",
    "\n",
    "        loss_dict = self.model(data)\n",
    "        losses = sum(loss_dict.values())\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "\n",
    "        self._trainer._write_metrics(loss_dict, data_time)\n",
    "\n",
    "        # Log training metrics\n",
    "        if self.iter % 50 == 0:  # Log every 50 iterations\n",
    "            self.training_history[\"iterations\"].append(self.iter)\n",
    "            self.training_history[\"total_loss\"].append(losses.item())\n",
    "            self.training_history[\"learning_rate\"].append(self.optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def save_training_history(self):\n",
    "        \"\"\"Save training history to file\"\"\"\n",
    "        history_file = os.path.join(self.cfg.OUTPUT_DIR, \"training_history.json\")\n",
    "        with open(history_file, \"w\") as f:\n",
    "            json.dump(self.training_history, f, indent=2)\n",
    "        print(f\"Training history saved to {history_file}\")\n",
    "\n",
    "print(\"Custom trainer class defined with:\")\n",
    "print(\"- Enhanced loss logging\")\n",
    "print(\"- Custom data augmentations\")\n",
    "print(\"- Training history tracking\")\n",
    "print(\"- Periodic validation evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFEd5xEX35sZ"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3468,
     "status": "error",
     "timestamp": 1754593268460,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "8Guv0AdI35sZ",
    "outputId": "92e32f0d-97bb-4174-81f2-9389fd7ecd66"
   },
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "print(\"Initializing trainer...\")\n",
    "trainer = HieroglyphTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "\n",
    "print(f\"Starting training for {cfg.SOLVER.MAX_ITER} iterations...\")\n",
    "print(f\"Model will be saved to: {output_dir}\")\n",
    "print(f\"Training started at: {datetime.now()}\")\n",
    "print(\"-\"* 60)\n",
    "\n",
    "# Start training\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"\\n Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Training failed with error: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    end_time = time.time()\n",
    "    training_duration = end_time - start_time\n",
    "    print(f\"\\nTraining duration: {training_duration/3600:.2f} hours\")\n",
    "\n",
    "    # Save training history\n",
    "    trainer.save_training_history()\n",
    "\n",
    "    # Save final model\n",
    "    trainer.checkpointer.save(\"model_final\")\n",
    "    print(f\"Final model saved to: {output_dir}/model_final.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7y3VY9k35sZ"
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YR7HEu9L35sZ"
   },
   "outputs": [],
   "source": [
    "# Load trained model for evaluation\n",
    "print(\"Loading trained model for evaluation...\")\n",
    "\n",
    "# Update config for inference\n",
    "cfg.MODEL.WEIGHTS = os.path.join(output_dir, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Detection threshold\n",
    "cfg.DATASETS.TEST = (\"hieroglyphs_val\",)\n",
    "\n",
    "# Create predictor\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Detection threshold: {cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST}\")\n",
    "\n",
    "# Run evaluation on validation set\n",
    "print(\"Running evaluation on validation set...\")\n",
    "\n",
    "evaluator = COCOEvaluator(\"hieroglyphs_val\", cfg, False, output_dir)\n",
    "val_loader = build_detection_test_loader(cfg, \"hieroglyphs_val\")\n",
    "\n",
    "eval_results = inference_on_dataset(predictor.model, val_loader, evaluator)\n",
    "\n",
    "print(\"\\n Validation Results:\")\n",
    "print(f\"AP (Average Precision): {eval_results[\"bbox\"][\"AP\"]:.3f}\")\n",
    "print(f\"AP50 (AP @ IoU=0.5): {eval_results[\"bbox\"][\"AP50\"]:.3f}\")\n",
    "print(f\"AP75 (AP @ IoU=0.75): {eval_results[\"bbox\"][\"AP75\"]:.3f}\")\n",
    "print(f\"APs (Small objects): {eval_results[\"bbox\"][\"APs\"]:.3f}\")\n",
    "print(f\"APm (Medium objects): {eval_results[\"bbox\"][\"APm\"]:.3f}\")\n",
    "print(f\"APl (Large objects): {eval_results[\"bbox\"][\"APl\"]:.3f}\")\n",
    "\n",
    "if \"segm\" in eval_results:\n",
    "    print(f\"\\nSegmentation Results:\")\n",
    "    print(f\"Mask AP: {eval_results[\"segm\"][\"AP\"]:.3f}\")\n",
    "    print(f\"Mask AP50: {eval_results[\"segm\"][\"AP50\"]:.3f}\")\n",
    "    print(f\"Mask AP75: {eval_results[\"segm\"][\"AP75\"]:.3f}\")\n",
    "\n",
    "# Save evaluation results\n",
    "eval_file = os.path.join(output_dir, \"validation_results.json\")\n",
    "with open(eval_file, \"w\") as f:\n",
    "    json.dump(eval_results, f, indent=2)\n",
    "print(f\"\\nValidation results saved to: {eval_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwhXC5nL35sa"
   },
   "source": [
    "## Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4X-r66h35sa"
   },
   "outputs": [],
   "source": [
    "# Create training summary\n",
    "training_summary = {\n",
    "    \"model_info\": {\n",
    "        \"architecture\": \"Mask R-CNN with ResNet-50 FPN\",\n",
    "        \"num_classes\": len(category_names),\n",
    "        \"training_iterations\": cfg.SOLVER.MAX_ITER,\n",
    "        \"batch_size\": cfg.SOLVER.IMS_PER_BATCH,\n",
    "        \"learning_rate\": cfg.SOLVER.BASE_LR,\n",
    "        \"timestamp\": timestamp\n",
    "    },\n",
    "    \"dataset_info\": {\n",
    "        \"train_annotations\": len(train_data[\"annotations\"]),\n",
    "        \"val_annotations\": len(val_data[\"annotations\"]),\n",
    "        \"test_annotations\": len(test_data[\"annotations\"]),\n",
    "        \"categories\": len(category_names),\n",
    "        \"category_id_remapping\": \"Applied - mapped from original COCO IDs to 0-indexed\"\n",
    "    },\n",
    "    \"training_results\": {\n",
    "        \"training_duration_hours\": training_duration / 3600 if \"training_duration\" in locals() else None,\n",
    "        \"final_loss\": trainer.training_history[\"total_loss\"][-1] if trainer.training_history[\"total_loss\"] else None\n",
    "    },\n",
    "    \"validation_results\": eval_results if \"eval_results\" in locals() else None,\n",
    "    \"files_created\": [\n",
    "        \"model_final.pth\",\n",
    "        \"config.yaml\",\n",
    "        \"training_history.json\",\n",
    "        \"validation_results.json\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save training summary\n",
    "summary_file = os.path.join(output_dir, \"training_summary.json\")\n",
    "with open(summary_file, \"w\") as f:\n",
    "    json.dump(training_summary, f, indent=2, default=str)\n",
    "\n",
    "# Create model info file for easy loading\n",
    "model_info = {\n",
    "    \"model_path\": os.path.join(output_dir, \"model_final.pth\"),\n",
    "    \"config_path\": os.path.join(output_dir, \"config.yaml\"),\n",
    "    \"num_classes\": len(category_names),\n",
    "    \"category_names\": category_names,\n",
    "    \"category_id_mapping\": category_id_map,\n",
    "    \"detection_threshold\": cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST,\n",
    "    \"timestamp\": timestamp\n",
    "}\n",
    "\n",
    "model_info_file = os.path.join(output_dir, \"model_info.json\")\n",
    "with open(model_info_file, \"w\") as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"* 60)\n",
    "print(f\"Model trained and saved to: {output_dir}\")\n",
    "print(f\"Training summary saved to: {summary_file}\")\n",
    "print(f\"Model info saved to: {model_info_file}\")\n",
    "\n",
    "if \"eval_results\" in locals():\n",
    "    print(f\"\\n Final Performance Metrics:\")\n",
    "    print(f\"AP (Average Precision): {eval_results[\"bbox\"][\"AP\"]:.3f}\")\n",
    "    print(f\"AP50: {eval_results[\"bbox\"][\"AP50\"]:.3f}\")\n",
    "    print(f\"AP75: {eval_results[\"bbox\"][\"AP75\"]:.3f}\")\n",
    "\n",
    "print(f\"\\n NEXT STEPS:\")\n",
    "print(f\"1. Run notebook 04_model_evaluation.ipynb for detailed evaluation\")\n",
    "print(f\"2. Test the model on the test set\")\n",
    "print(f\"3. Analyze failure cases and model performance\")\n",
    "print(f\"4. Consider fine-tuning or adjusting hyperparameters if needed\")\n",
    "\n",
    "print(f\"\\n Output Directory Contents:\")\n",
    "for file in os.listdir(output_dir):\n",
    "    file_path = os.path.join(output_dir, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        print(f\"{file}: {size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12612380,
     "status": "ok",
     "timestamp": 1754605918648,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "0c43659a",
    "outputId": "4cc41e5d-d733-4c8b-ba7f-56332e62cd81"
   },
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "print(\"Initializing trainer...\")\n",
    "trainer = HieroglyphTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "\n",
    "print(f\"Starting training for {cfg.SOLVER.MAX_ITER} iterations...\")\n",
    "print(f\"Model will be saved to: {output_dir}\")\n",
    "print(f\"Training started at: {datetime.now()}\")\n",
    "print(\"-\"* 60)\n",
    "\n",
    "# Start training\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"\\n Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Training failed with error: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    end_time = time.time()\n",
    "    training_duration = end_time - start_time\n",
    "    print(f\"\\nTraining duration: {training_duration/3600:.2f} hours\")\n",
    "\n",
    "    # Save training history\n",
    "    trainer.save_training_history()\n",
    "\n",
    "    # Save final model\n",
    "    trainer.checkpointer.save(\"model_final\")\n",
    "    print(f\"Final model saved to: {output_dir}/model_final.pth\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
