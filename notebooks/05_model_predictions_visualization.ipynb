{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ue1tavWEb5XG"
   },
   "source": [
    "# PapyrusNU Hieroglyph Detection - Model Predictions Visualization\n",
    "\n",
    "This notebook creates visualizations of your trained model\"s predictions on the test set.\n",
    "\n",
    "## What you\"ll see:\n",
    "1. **Side-by-side comparisons**: Ground truth vs predictions\n",
    "2. **Confidence score analysis**: How confident is the model?\n",
    "3. **Class-specific performance**: Which hieroglyphs work best?\n",
    "4. **Detection overlays**: Visual prediction results\n",
    "5. **Performance metrics**: Detailed analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 20180,
     "status": "ok",
     "timestamp": 1754653280675,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "9cEkpbPmb8CD",
    "outputId": "c3d2cfbf-821f-449f-9f8b-9b779b153087"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -U torch torchvision cython\n",
    "!pip install -U \"git+https://github.com/facebookresearch/fvcore.git\"\"git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\"\n",
    "import torch, torchvision\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12196,
     "status": "ok",
     "timestamp": 1754653292874,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "x7dl1qY3b76Q",
    "outputId": "08af69a3-301c-4425-c5e9-8c8442d76ea6"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/facebookresearch/detectron2 detectron2_repo\n",
    "!pip install -e detectron2_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2728,
     "status": "ok",
     "timestamp": 1754653295621,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "UvQ7oCAJb5XI",
    "outputId": "b1d5b5b0-95dc-4287-f503-0da728b27238"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Set up paths\n",
    "PROJECT_ROOT = \"/content/drive/My Drive/PapyrusNU_Detectron\"\n",
    "DATA_DIR = f\"{PROJECT_ROOT}/data\"\n",
    "MODELS_DIR = f\"{PROJECT_ROOT}/models\"\n",
    "SCRIPTS_DIR = f\"{PROJECT_ROOT}/scripts\"\n",
    "\n",
    "sys.path.append(SCRIPTS_DIR)\n",
    "\n",
    "print(\"Model Prediction Visualization Setup Complete!\")\n",
    "print(f\"Project: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1754653295640,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "cQ-rXKMLcOdP",
    "outputId": "4cac7fef-2e06-4508-f38b-0bd8244afc28"
   },
   "outputs": [],
   "source": [
    "print(\"Current system path:\")\n",
    "for path in sys.path:\n",
    "    print(path)\n",
    "\n",
    "print(\"\\nContents of scripts directory:\")\n",
    "scripts_dir = \"/content/drive/My Drive/PapyrusNU_Detectron/scripts\"\n",
    "if os.path.exists(scripts_dir):\n",
    "    for item in os.listdir(scripts_dir):\n",
    "        print(item)\n",
    "else:\n",
    "    print(f\"Directory not found: {scripts_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 909,
     "status": "ok",
     "timestamp": 1754653296571,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "MSHhZs0Ub5XI",
    "outputId": "7f1bd872-59b0-4a82-b6f8-e0f35656f00d"
   },
   "outputs": [],
   "source": [
    "# Import Detectron2 components\n",
    "import detectron2\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "# Import custom utilities\n",
    "from dataset_utils import create_detectron2_dataset_dict\n",
    "\n",
    "print(f\"Detectron2 version: {detectron2.__version__}\")\n",
    "print(\"Custom utilities loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgS-7eUvb5XJ"
   },
   "source": [
    "## Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 888,
     "status": "ok",
     "timestamp": 1754653300704,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "7CiESAymb5XJ",
    "outputId": "82a419d9-91d0-4cc5-e84e-535335d47d33"
   },
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model_dirs = glob.glob(f\"{MODELS_DIR}/hieroglyph_model_*\")\n",
    "latest_model_dir = sorted(model_dirs)[-1]\n",
    "\n",
    "print(f\"📁 Loading model: {os.path.basename(latest_model_dir)}\")\n",
    "\n",
    "# Load model info\n",
    "with open(f\"{latest_model_dir}/model_info.json\", \"r\") as f:\n",
    "    model_info = json.load(f)\n",
    "\n",
    "# Load test data\n",
    "with open(f\"{DATA_DIR}/annotations/test_annotations.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "test_dataset_dicts = create_detectron2_dataset_dict(test_data, f\"{DATA_DIR}/images\")\n",
    "\n",
    "print(f\"Model Info:\")\n",
    "print(f\"Classes: {model_info[\"num_classes\"]}\")\n",
    "print(f\"Test images: {len(test_dataset_dicts)}\")\n",
    "print(f\"Test annotations: {len(test_data[\"annotations\"])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9432,
     "status": "ok",
     "timestamp": 1754653312675,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "FbcqZMdrb5XJ",
    "outputId": "d5057ba7-093c-40f2-8abc-034b65cf9d3d"
   },
   "outputs": [],
   "source": [
    "# Set up model for CPU prediction\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(f\"{latest_model_dir}/config.yaml\")\n",
    "cfg.MODEL.WEIGHTS = f\"{latest_model_dir}/model_final.pth\"\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = model_info[\"detection_threshold\"]\n",
    "cfg.MODEL.DEVICE = \"cpu\"# Force CPU usage\n",
    "\n",
    "print(\"Loading model for prediction (CPU mode)...\")\n",
    "predictor = DefaultPredictor(cfg)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgBUvinwb5XJ"
   },
   "source": [
    "## Generate Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3586,
     "status": "ok",
     "timestamp": 1754653318574,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "jLY0NMNvb5XJ",
    "outputId": "3b8c12b5-8b8e-4b29-901f-64fed14f8232"
   },
   "outputs": [],
   "source": [
    "# Generate predictions for all test images\n",
    "all_predictions = []\n",
    "all_ground_truth = []\n",
    "\n",
    "print(\"Generating predictions on test set...\")\n",
    "\n",
    "for i, dataset_dict in enumerate(test_dataset_dicts):\n",
    "    print(f\"Processing image {i+1}/{len(test_dataset_dicts)}: {os.path.basename(dataset_dict[\"file_name\"])}\")\n",
    "\n",
    "    # Load image\n",
    "    img = cv2.imread(dataset_dict[\"file_name\"])\n",
    "\n",
    "    # Make prediction\n",
    "    outputs = predictor(img)\n",
    "    instances = outputs[\"instances\"].to(\"cpu\")\n",
    "\n",
    "    # Store predictions\n",
    "    image_predictions = {\n",
    "        \"image_id\": dataset_dict[\"image_id\"],\n",
    "        \"file_name\": dataset_dict[\"file_name\"],\n",
    "        \"image_shape\": img.shape,\n",
    "        \"predictions\": {\n",
    "            \"boxes\": instances.pred_boxes.tensor.numpy() if len(instances) > 0 else np.array([]),\n",
    "            \"scores\": instances.scores.numpy() if len(instances) > 0 else np.array([]),\n",
    "            \"classes\": instances.pred_classes.numpy() if len(instances) > 0 else np.array([]),\n",
    "            \"masks\": instances.pred_masks.numpy() if len(instances) > 0 and hasattr(instances, \"pred_masks\") else None\n",
    "        },\n",
    "        \"ground_truth\": dataset_dict[\"annotations\"]\n",
    "    }\n",
    "\n",
    "    all_predictions.append(image_predictions)\n",
    "\n",
    "    print(f\"Detections: {len(instances)}, Ground truth: {len(dataset_dict[\"annotations\"])}\")\n",
    "\n",
    "print(f\"\\n Prediction generation complete!\")\n",
    "print(f\"Total predictions across all images: {sum(len(pred[\"predictions\"][\"scores\"]) for pred in all_predictions)}\")\n",
    "print(f\"Total ground truth annotations: {sum(len(pred[\"ground_truth\"]) for pred in all_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ngFLd0Kb5XK"
   },
   "source": [
    "## Visualization 1: Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571,
     "output_embedded_package_id": "1G-9HwTUz-s2kLdJwxsrgvbVFmSkg4ipT"
    },
    "executionInfo": {
     "elapsed": 14771,
     "status": "ok",
     "timestamp": 1754653336614,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "ym14MCuRb5XK",
    "outputId": "04fad898-2f22-4131-a46c-8d886eada036"
   },
   "outputs": [],
   "source": [
    "# Create side-by-side visualizations for each test image\n",
    "def draw_annotations_on_image(img, annotations, category_names, title_prefix=\"\", color=\"red\"):\n",
    "    \"\"\"Draw bounding boxes for annotations\"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    for ann in annotations:\n",
    "        if \"bbox\"in ann:\n",
    "            # Ground truth format: [x, y, width, height]\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            rect = Rectangle((x, y), w, h, linewidth=2, edgecolor=color, facecolor=\"none\")\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # Add class label\n",
    "            if \"category_id\"in ann and ann[\"category_id\"] < len(category_names):\n",
    "                class_name = category_names[ann[\"category_id\"]]\n",
    "                ax.text(x, y-5, class_name, color=color, fontsize=8, weight=\"bold\")\n",
    "\n",
    "    ax.set_title(f\"{title_prefix} ({len(annotations)} annotations)\")\n",
    "    ax.axis(\"off\")\n",
    "    return fig, ax\n",
    "\n",
    "def draw_predictions_on_image(img, predictions, category_names, title_prefix=\"\", color=\"blue\"):\n",
    "    \"\"\"Draw bounding boxes for predictions\"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    boxes = predictions[\"boxes\"]\n",
    "    scores = predictions[\"scores\"]\n",
    "    classes = predictions[\"classes\"]\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        # Prediction format: [x1, y1, x2, y2]\n",
    "        x1, y1, x2, y2 = boxes[i]\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "\n",
    "        # Color based on confidence\n",
    "        confidence = scores[i]\n",
    "        alpha = 0.3 + 0.7 * confidence  # Higher confidence = more opaque\n",
    "\n",
    "        rect = Rectangle((x1, y1), w, h, linewidth=2, edgecolor=color,\n",
    "                        facecolor=\"none\", alpha=alpha)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Add class label and confidence\n",
    "        class_idx = int(classes[i])\n",
    "        if class_idx < len(category_names):\n",
    "            class_name = category_names[class_idx]\n",
    "            label = f\"{class_name} ({confidence:.2f})\"\n",
    "            ax.text(x1, y1-5, label, color=color, fontsize=8, weight=\"bold\")\n",
    "\n",
    "    ax.set_title(f\"{title_prefix} ({len(boxes)} detections)\")\n",
    "    ax.axis(\"off\")\n",
    "    return fig, ax\n",
    "\n",
    "print(\"Creating side-by-side visualizations...\")\n",
    "\n",
    "# Create visualizations for each test image\n",
    "for i, pred_data in enumerate(all_predictions):\n",
    "    img = cv2.imread(pred_data[\"file_name\"])\n",
    "    image_name = os.path.basename(pred_data[\"file_name\"])\n",
    "\n",
    "    # Create side-by-side comparison\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 12))\n",
    "\n",
    "    # Ground Truth (Left)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    ax1.imshow(img_rgb)\n",
    "\n",
    "    # Draw ground truth annotations\n",
    "    for ann in pred_data[\"ground_truth\"]:\n",
    "        x, y, w, h = ann[\"bbox\"]\n",
    "        rect = Rectangle((x, y), w, h, linewidth=3, edgecolor=\"red\", facecolor=\"none\")\n",
    "        ax1.add_patch(rect)\n",
    "\n",
    "        # Add ground truth label\n",
    "        if ann[\"category_id\"] < len(model_info[\"category_names\"]):\n",
    "            class_name = model_info[\"category_names\"][ann[\"category_id\"]]\n",
    "            ax1.text(x, y-10, class_name, color=\"red\", fontsize=10, weight=\"bold\",\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "    ax1.set_title(f\"Ground Truth: {image_name}\\n{len(pred_data[\"ground_truth\"])} annotations\", fontsize=14)\n",
    "    ax1.axis(\"off\")\n",
    "\n",
    "    # Predictions (Right)\n",
    "    ax2.imshow(img_rgb)\n",
    "\n",
    "    # Draw predictions\n",
    "    boxes = pred_data[\"predictions\"][\"boxes\"]\n",
    "    scores = pred_data[\"predictions\"][\"scores\"]\n",
    "    classes = pred_data[\"predictions\"][\"classes\"]\n",
    "\n",
    "    for j in range(len(boxes)):\n",
    "        x1, y1, x2, y2 = boxes[j]\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        confidence = scores[j]\n",
    "\n",
    "        # Color based on confidence: green for high, yellow for medium, orange for low\n",
    "        if confidence >= 0.8:\n",
    "            color = \"green\"\n",
    "        elif confidence >= 0.6:\n",
    "            color = \"blue\"\n",
    "        else:\n",
    "            color = \"orange\"\n",
    "\n",
    "        rect = Rectangle((x1, y1), w, h, linewidth=3, edgecolor=color, facecolor=\"none\")\n",
    "        ax2.add_patch(rect)\n",
    "\n",
    "        # Add prediction label\n",
    "        class_idx = int(classes[j])\n",
    "        if class_idx < len(model_info[\"category_names\"]):\n",
    "            class_name = model_info[\"category_names\"][class_idx]\n",
    "            label = f\"{class_name}\\n{confidence:.3f}\"\n",
    "            ax2.text(x1, y1-15, label, color=color, fontsize=10, weight=\"bold\",\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "    # Add confidence color legend\n",
    "    legend_elements = [\n",
    "        plt.Rectangle((0,0),1,1, facecolor=\"green\", alpha=0.3, label=\"High confidence (≥0.8)\"),\n",
    "        plt.Rectangle((0,0),1,1, facecolor=\"blue\", alpha=0.3, label=\"Medium confidence (0.6-0.8)\"),\n",
    "        plt.Rectangle((0,0),1,1, facecolor=\"orange\", alpha=0.3, label=\"Low confidence (<0.6)\")\n",
    "    ]\n",
    "    ax2.legend(handles=legend_elements, loc=\"upper right\")\n",
    "\n",
    "    ax2.set_title(f\"Model Predictions: {image_name}\\n{len(boxes)} detections\", fontsize=14)\n",
    "    ax2.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(f\"Hieroglyph Detection Comparison - Image {i+1}\", fontsize=16, y=0.95)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the visualization\n",
    "    output_path = f\"{latest_model_dir}/prediction_comparison_{i+1}.png\"\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Saved: {output_path}\")\n",
    "\n",
    "print(\"Side-by-side visualizations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRwOh1zfb5XK"
   },
   "source": [
    "## Visualization 2: Confidence Score Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 976
    },
    "executionInfo": {
     "elapsed": 1512,
     "status": "ok",
     "timestamp": 1754653373901,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "tb9soeKTb5XK",
    "outputId": "58079671-10b2-463c-d939-5f30dfc80c27"
   },
   "outputs": [],
   "source": [
    "# Analyze confidence scores across all predictions\n",
    "all_scores = []\n",
    "all_classes = []\n",
    "class_scores = defaultdict(list)\n",
    "\n",
    "for pred_data in all_predictions:\n",
    "    scores = pred_data[\"predictions\"][\"scores\"]\n",
    "    classes = pred_data[\"predictions\"][\"classes\"]\n",
    "\n",
    "    all_scores.extend(scores)\n",
    "    all_classes.extend(classes)\n",
    "\n",
    "    for score, class_idx in zip(scores, classes):\n",
    "        class_name = model_info[\"category_names\"][int(class_idx)]\n",
    "        class_scores[class_name].append(score)\n",
    "\n",
    "if all_scores:\n",
    "    # Create confidence analysis plots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 12))\n",
    "\n",
    "    # 1. Confidence distribution histogram\n",
    "    ax1.hist(all_scores, bins=20, alpha=0.7, color=\"skyblue\", edgecolor=\"black\")\n",
    "    ax1.axvline(np.mean(all_scores), color=\"red\", linestyle=\"--\", linewidth=2, label=f\"Mean: {np.mean(all_scores):.3f}\")\n",
    "    ax1.axvline(np.median(all_scores), color=\"green\", linestyle=\"--\", linewidth=2, label=f\"Median: {np.median(all_scores):.3f}\")\n",
    "    ax1.set_xlabel(\"Confidence Score\")\n",
    "    ax1.set_ylabel(\"Number of Detections\")\n",
    "    ax1.set_title(\"Distribution of Confidence Scores\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Confidence by threshold\n",
    "    thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    counts = [sum(1 for s in all_scores if s >= thresh) for thresh in thresholds]\n",
    "    percentages = [c/len(all_scores)*100 for c in counts]\n",
    "\n",
    "    bars = ax2.bar([f\"≥{t}\"for t in thresholds], counts, alpha=0.7, color=\"lightcoral\")\n",
    "    ax2.set_ylabel(\"Number of Detections\")\n",
    "    ax2.set_title(\"Detections by Confidence Threshold\")\n",
    "\n",
    "    # Add percentage labels on bars\n",
    "    for bar, pct in zip(bars, percentages):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f\"{pct:.1f}%\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    # 3. Top detected classes with confidence\n",
    "    class_counts = Counter(model_info[\"category_names\"][int(c)] for c in all_classes)\n",
    "    top_classes = class_counts.most_common(10)\n",
    "\n",
    "    if top_classes:\n",
    "        class_names = [item[0] for item in top_classes]\n",
    "        class_detection_counts = [item[1] for item in top_classes]\n",
    "\n",
    "        bars = ax3.bar(range(len(class_names)), class_detection_counts, alpha=0.7, color=\"lightgreen\")\n",
    "        ax3.set_xticks(range(len(class_names)))\n",
    "        ax3.set_xticklabels(class_names, rotation=45, ha=\"right\")\n",
    "        ax3.set_ylabel(\"Number of Detections\")\n",
    "        ax3.set_title(\"Top 10 Most Detected Hieroglyph Classes\")\n",
    "\n",
    "        # Add count labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                    f\"{int(height)}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    # 4. Average confidence by class (for classes with multiple detections)\n",
    "    classes_with_multiple = {k: v for k, v in class_scores.items() if len(v) >= 2}\n",
    "    if classes_with_multiple:\n",
    "        sorted_classes = sorted(classes_with_multiple.items(),\n",
    "                              key=lambda x: np.mean(x[1]), reverse=True)[:10]\n",
    "\n",
    "        class_names = [item[0] for item in sorted_classes]\n",
    "        avg_confidences = [np.mean(item[1]) for item in sorted_classes]\n",
    "        std_confidences = [np.std(item[1]) for item in sorted_classes]\n",
    "\n",
    "        bars = ax4.bar(range(len(class_names)), avg_confidences,\n",
    "                      yerr=std_confidences, alpha=0.7, color=\"gold\", capsize=5)\n",
    "        ax4.set_xticks(range(len(class_names)))\n",
    "        ax4.set_xticklabels(class_names, rotation=45, ha=\"right\")\n",
    "        ax4.set_ylabel(\"Average Confidence Score\")\n",
    "        ax4.set_title(\"Average Confidence by Hieroglyph Class\\n(Classes with ≥2 detections)\")\n",
    "\n",
    "        # Add confidence labels\n",
    "        for bar, conf in zip(bars, avg_confidences):\n",
    "            height = bar.get_height()\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                    f\"{conf:.3f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "    plt.suptitle(\"Model Prediction Confidence Analysis\", fontsize=16, y=0.98)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the analysis\n",
    "    confidence_analysis_path = f\"{latest_model_dir}/confidence_analysis.png\"\n",
    "    plt.savefig(confidence_analysis_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Confidence analysis saved: {confidence_analysis_path}\")\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(\"\\n CONFIDENCE SCORE SUMMARY:\")\n",
    "    print(f\"Total detections: {len(all_scores)}\")\n",
    "    print(f\"Mean confidence: {np.mean(all_scores):.3f}\")\n",
    "    print(f\"Median confidence: {np.median(all_scores):.3f}\")\n",
    "    print(f\"Standard deviation: {np.std(all_scores):.3f}\")\n",
    "    print(f\"Min confidence: {np.min(all_scores):.3f}\")\n",
    "    print(f\"Max confidence: {np.max(all_scores):.3f}\")\n",
    "    print(f\"High confidence detections (≥0.8): {sum(1 for s in all_scores if s >= 0.8)} ({sum(1 for s in all_scores if s >= 0.8)/len(all_scores)*100:.1f}%)\")\n",
    "\n",
    "else:\n",
    "    print(\"No predictions found to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPdZb9Z5b5XL"
   },
   "source": [
    "## Visualization 3: Detection Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1754653387247,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "rNd_2IIjb5XL",
    "outputId": "7f8ec20f-537b-4ff4-9850-b48bb60e8d70"
   },
   "outputs": [],
   "source": [
    "# Create a comprehensive summary report\n",
    "print(\"COMPREHENSIVE DETECTION SUMMARY REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_predictions = sum(len(pred[\"predictions\"][\"scores\"]) for pred in all_predictions)\n",
    "total_ground_truth = sum(len(pred[\"ground_truth\"]) for pred in all_predictions)\n",
    "unique_predicted_classes = len(set(all_classes))\n",
    "unique_gt_classes = len(set(ann[\"category_id\"] for pred in all_predictions for ann in pred[\"ground_truth\"]))\n",
    "\n",
    "print(f\"\\n OVERALL PERFORMANCE:\")\n",
    "print(f\"Test Images: {len(all_predictions)}\")\n",
    "print(f\"Total Predictions: {total_predictions}\")\n",
    "print(f\"Total Ground Truth: {total_ground_truth}\")\n",
    "print(f\"Detection Rate: {total_predictions/total_ground_truth:.2%}\")\n",
    "print(f\"Unique Classes Predicted: {unique_predicted_classes}\")\n",
    "print(f\"Unique Classes in GT: {unique_gt_classes}\")\n",
    "print(f\"Class Coverage: {unique_predicted_classes/unique_gt_classes:.2%}\")\n",
    "\n",
    "if all_scores:\n",
    "    print(f\"\\n CONFIDENCE METRICS:\")\n",
    "    print(f\"Mean Confidence: {np.mean(all_scores):.3f}\")\n",
    "    print(f\"High Confidence (≥0.8): {sum(1 for s in all_scores if s >= 0.8)} detections ({sum(1 for s in all_scores if s >= 0.8)/len(all_scores)*100:.1f}%)\")\n",
    "    print(f\"Medium Confidence (0.6-0.8): {sum(1 for s in all_scores if 0.6 <= s < 0.8)} detections\")\n",
    "    print(f\"Low Confidence (<0.6): {sum(1 for s in all_scores if s < 0.6)} detections\")\n",
    "\n",
    "print(f\"\\n TOP DETECTED CLASSES:\")\n",
    "class_counts = Counter(model_info[\"category_names\"][int(c)] for c in all_classes)\n",
    "for class_name, count in class_counts.most_common(10):\n",
    "    avg_conf = np.mean([score for score, class_idx in zip(all_scores, all_classes)\n",
    "                       if model_info[\"category_names\"][int(class_idx)] == class_name])\n",
    "    print(f\"{class_name}: {count} detections (avg conf: {avg_conf:.3f})\")\n",
    "\n",
    "print(f\"\\n SAVED VISUALIZATIONS:\")\n",
    "saved_files = []\n",
    "for i in range(len(all_predictions)):\n",
    "    file_path = f\"{latest_model_dir}/prediction_comparison_{i+1}.png\"\n",
    "    if os.path.exists(file_path):\n",
    "        saved_files.append(f\"prediction_comparison_{i+1}.png\")\n",
    "\n",
    "if os.path.exists(f\"{latest_model_dir}/confidence_analysis.png\"):\n",
    "    saved_files.append(\"confidence_analysis.png\")\n",
    "\n",
    "for file in saved_files:\n",
    "    print(f\"{file}\")\n",
    "\n",
    "print(f\"\\n All visualizations saved to: {latest_model_dir}/\")\n",
    "print(\"=\"*60)\n",
    "print(\"VISUALIZATION COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
