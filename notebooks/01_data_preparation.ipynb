{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfxdpQOoUjE3"
   },
   "source": [
    "# PapyrusVision Hieroglyph Detection - Data Preparation\n",
    "\n",
    "This notebook handles data preparation for training a Detectron2 model to identify hieroglyphs in Papyrus Nu.\n",
    "\n",
    "## Objectives:\n",
    "1. Load and validate the dataset\n",
    "2. Analyze dataset statistics\n",
    "3. Split data into train/val/test sets\n",
    "4. Prepare data for Detectron2 training\n",
    "5. Visualize sample annotations\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 204554,
     "status": "ok",
     "timestamp": 1754588483296,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "Vd_jacKlnCAs",
    "outputId": "57265a78-ccd4-45b8-d3fb-9fbd2238d8f1"
   },
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip install -U torch torchvision cython\n",
    "!pip install -U 'git+https://github.com/facebookresearch/fvcore.git' 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
    "import torch, torchvision\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11476,
     "status": "ok",
     "timestamp": 1754588984593,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "Ht39qMyTnFIe",
    "outputId": "875839a0-4973-411a-8aef-2e26387475b9"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/facebookresearch/detectron2 detectron2_repo\n",
    "!pip install -e detectron2_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5642,
     "status": "ok",
     "timestamp": 1754589016879,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "PZlxhqFMX6w1",
    "outputId": "476d59a1-29da-41c6-93b2-066623c65565"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "\n",
    "# Check CUDA version to pick the correct Detectron2 wheel\n",
    "cuda_version = torch.version.cuda\n",
    "print(f\"Detected CUDA version: {cuda_version}\")\n",
    "\n",
    "# Install other required packages that are not included with the detectron2 installation command\n",
    "print(\"\\nInstalling other required packages...\")\n",
    "other_packages = [\n",
    "    'opencv-python-headless',\n",
    "    'matplotlib>=3.3.0',\n",
    "    'seaborn>=0.11.0',\n",
    "    'plotly>=5.0.0',\n",
    "    'pandas>=1.3.0',\n",
    "    'scikit-learn>=1.0.0',\n",
    "    'kaleido>=0.2.1',\n",
    "    'pycocotools',\n",
    "    'pillow>=8.0.0',\n",
    "    'numpy',\n",
    "    'tqdm',\n",
    "    'ipywidgets'\n",
    "]\n",
    "\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q'] + other_packages)\n",
    "    print(\"All other packages installed successfully!\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Failed to install other packages: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Installation of other packages failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5693,
     "status": "ok",
     "timestamp": 1754589022574,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "XJ-zV5OjUjE5",
    "outputId": "f3638d60-3c34-492c-9da4-a8b62ae8e202"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from google.colab import drive, files\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set up paths\n",
    "PROJECT_ROOT = '/content/drive/My Drive/PapyrusNU_Detectron'\n",
    "\n",
    "\n",
    "DATA_DIR = f'{PROJECT_ROOT}/data'\n",
    "SCRIPTS_DIR = f'{PROJECT_ROOT}/scripts'\n",
    "MODELS_DIR = f'{PROJECT_ROOT}/models'\n",
    "NOTEBOOKS_DIR = f'{PROJECT_ROOT}/notebooks'\n",
    "\n",
    "# Add scripts to path\n",
    "sys.path.append(SCRIPTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOzvho0KUjE6"
   },
   "source": [
    "## Data Loading and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1754589022911,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "7b11fa52",
    "outputId": "670a974f-65bd-4afd-cd47-13d80a3a7bce"
   },
   "outputs": [],
   "source": [
    "print(\"Current system path:\")\n",
    "for path in sys.path:\n",
    "    print(path)\n",
    "\n",
    "print(\"\\nContents of scripts directory:\")\n",
    "scripts_dir = '/content/drive/My Drive/PapyrusNU_Detectron/scripts'\n",
    "if os.path.exists(scripts_dir):\n",
    "    for item in os.listdir(scripts_dir):\n",
    "        print(item)\n",
    "else:\n",
    "    print(f\"Directory not found: {scripts_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1754589023261,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "HQ2v0Bo-UjE6",
    "outputId": "4bddb9be-cc8a-405a-9098-70ced6557d53"
   },
   "outputs": [],
   "source": [
    "# Import our custom utilities\n",
    "from dataset_utils import HieroglyphDatasetUtils\n",
    "from visualization import HieroglyphVisualizer\n",
    "\n",
    "# Initialize dataset utilities\n",
    "annotation_file = f\"{DATA_DIR}/annotations/annotations.json\"\n",
    "dataset_utils = HieroglyphDatasetUtils(annotation_file)\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Number of images: {len(dataset_utils.images)}\")\n",
    "print(f\"Number of annotations: {len(dataset_utils.annotations)}\")\n",
    "print(f\"Number of categories: {len(dataset_utils.categories)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 180,
     "status": "ok",
     "timestamp": 1754589026295,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "IaV-Rb3xUjE6",
    "outputId": "a6131fea-034a-4da8-ef63-f967db8f64cf"
   },
   "outputs": [],
   "source": [
    "# Validate annotations\n",
    "print(\"Validating annotations...\")\n",
    "issues = dataset_utils.validate_annotations()\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "for issue_type, issue_list in issues.items():\n",
    "    if issue_list:\n",
    "        print(f\"{issue_type}: {len(issue_list)} issues\")\n",
    "        if len(issue_list) <= 5:\n",
    "            print(f\"Sample IDs: {issue_list}\")\n",
    "        else:\n",
    "            print(f\"Sample IDs: {issue_list[:5]} ... (and {len(issue_list)-5} more)\")\n",
    "    else:\n",
    "        print(f\"{issue_type}: No issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTcVmKPAUjE6"
   },
   "source": [
    "## Dataset Statistics and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 362,
     "status": "ok",
     "timestamp": 1754589029713,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "_P3o8W43UjE7",
    "outputId": "27e5090f-933d-4d9b-9110-440eca99c36b"
   },
   "outputs": [],
   "source": [
    "# Get dataset statistics\n",
    "print(\"Computing dataset statistics...\")\n",
    "stats = dataset_utils.get_dataset_stats()\n",
    "\n",
    "print(\"\\n Dataset Overview:\")\n",
    "print(f\"Images: {stats['num_images']:,}\")\n",
    "print(f\"Annotations: {stats['num_annotations']:,}\")\n",
    "print(f\"Categories: {stats['num_categories']:,}\")\n",
    "print(f\"Unique Unicode codes: {stats['unique_unicode_codes']:,}\")\n",
    "print(f\"Unique Gardiner codes: {stats['unique_gardiner_codes']:,}\")\n",
    "\n",
    "print(\"\\n Area Statistics:\")\n",
    "area_stats = stats['area_stats']\n",
    "print(f\"Mean area: {area_stats['mean']:.1f} pixels²\")\n",
    "print(f\"Median area: {area_stats['median']:.1f} pixels²\")\n",
    "print(f\"Min area: {area_stats['min']:.1f} pixels²\")\n",
    "print(f\"Max area: {area_stats['max']:.1f} pixels²\")\n",
    "print(f\"Std deviation: {area_stats['std']:.1f} pixels²\")\n",
    "\n",
    "print(\"\\n Most Common Categories:\")\n",
    "for i, (cat_id, count) in enumerate(stats['most_common_categories'][:10], 1):\n",
    "    cat_name = dataset_utils.categories[cat_id]['name']\n",
    "    print(f\"{i:2d}. {cat_name}: {count} annotations\")\n",
    "\n",
    "print(\"\\n Least Common Categories:\")\n",
    "for i, (cat_id, count) in enumerate(stats['least_common_categories'][:5], 1):\n",
    "    cat_name = dataset_utils.categories[cat_id]['name']\n",
    "    print(f\"{i:2d}. {cat_name}: {count} annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 824,
     "status": "ok",
     "timestamp": 1754589032301,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "HOA3-iV3UjE7",
    "outputId": "dbe3adfd-2da5-4c64-eee5-a45e12340560"
   },
   "outputs": [],
   "source": [
    "# Analyze class balance\n",
    "print(\"Analyzing class balance...\")\n",
    "class_balance = dataset_utils.analyze_class_balance()\n",
    "\n",
    "print(f\"\\n Class Balance Analysis:\")\n",
    "print(f\"Total classes: {class_balance['num_classes']}\")\n",
    "print(f\"Frequent classes (>5%): {len(class_balance['frequent_classes'])}\")\n",
    "print(f\"Common classes (1-5%): {len(class_balance['common_classes'])}\")\n",
    "print(f\"Rare classes (<1%): {len(class_balance['rare_classes'])}\")\n",
    "\n",
    "print(\"\\n Most Frequent Classes:\")\n",
    "for name, count, percentage in class_balance['frequent_classes'][:5]:\n",
    "    print(f\"{name}: {count} annotations ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\n Some Rare Classes:\")\n",
    "for name, count, percentage in class_balance['rare_classes'][-5:]:\n",
    "    print(f\"{name}: {count} annotations ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 348,
     "status": "ok",
     "timestamp": 1754589034551,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "WqZZ8zZuUjE7",
    "outputId": "d85c1b62-b7e1-40ad-f753-fc7ec6a50307"
   },
   "outputs": [],
   "source": [
    "# Get Gardiner to Unicode mapping\n",
    "print(\"Creating Gardiner to Unicode mapping...\")\n",
    "gardiner_mapping = dataset_utils.get_gardiner_mapping()\n",
    "\n",
    "print(f\"\\n Gardiner Code Analysis:\")\n",
    "print(f\"Total Gardiner codes: {len(gardiner_mapping)}\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\n Sample Gardiner to Unicode mappings:\")\n",
    "sample_codes = list(gardiner_mapping.items())[:10]\n",
    "for gardiner_code, info in sample_codes:\n",
    "    unicode_codes = info['unicode_codes']\n",
    "    count = info['count']\n",
    "    unicode_str = ', '.join(unicode_codes) if unicode_codes else 'No Unicode'\n",
    "    print(f\"{gardiner_code}: {unicode_str} ({count} annotations)\")\n",
    "\n",
    "# Save mapping for later use\n",
    "mapping_file = f\"{DATA_DIR}/annotations/gardiner_unicode_mapping.json\"\n",
    "with open(mapping_file, 'w') as f:\n",
    "    json.dump(gardiner_mapping, f, indent=2)\n",
    "print(f\"\\n Mapping saved to {mapping_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYTG9fGZUjE7"
   },
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 951
    },
    "executionInfo": {
     "elapsed": 4822,
     "status": "ok",
     "timestamp": 1754589042057,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "DThQd03VUjE7",
    "outputId": "f2d0b634-0234-4362-ac6e-17832c07a5c1"
   },
   "outputs": [],
   "source": [
    "# Initialize visualizer\n",
    "visualizer = HieroglyphVisualizer(figsize=(15, 10))\n",
    "\n",
    "# Create the directory for analysis plots if it doesn't exist\n",
    "analysis_plots_dir = f\"{DATA_DIR}/analysis_plots\"\n",
    "os.makedirs(analysis_plots_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Create dataset overview plots\n",
    "print(\"Creating dataset overview plots...\")\n",
    "overview_fig = visualizer.plot_dataset_overview(stats,\n",
    "                                               save_path=f\"{analysis_plots_dir}/dataset_overview.png\")\n",
    "plt.show()\n",
    "\n",
    "# Create class distribution plots\n",
    "print(\"\\nCreating class distribution plots...\")\n",
    "class_dist_fig = visualizer.plot_class_distribution(class_balance,\n",
    "                                                   save_path=f\"{analysis_plots_dir}/class_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419,
     "output_embedded_package_id": "1i_5I7pgpmxH7LLPYAq3Afyx6rqb8-mLk"
    },
    "executionInfo": {
     "elapsed": 13521,
     "status": "ok",
     "timestamp": 1754589055579,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "mOsoGPM5UjE7",
    "outputId": "7fdf4148-53e6-4389-e2db-84e9d83cf712"
   },
   "outputs": [],
   "source": [
    "# Visualize annotations on the image\n",
    "image_path = f\"{DATA_DIR}/images/145_upscaled_bright.jpg\"\n",
    "\n",
    "print(\"Creating annotation visualization...\")\n",
    "# Show a subset of annotations to avoid clutter\n",
    "annotation_fig = visualizer.visualize_annotations_on_image(\n",
    "    image_path=image_path,\n",
    "    annotations=dataset_utils.annotations[:100],  # First 100 annotations\n",
    "    categories=dataset_utils.categories,\n",
    "    max_annotations=50,  # Limit displayed annotations\n",
    "    save_path=f\"{DATA_DIR}/analysis_plots/sample_annotations.png\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Note: Showing first 50 of {len(dataset_utils.annotations)} total annotations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZEPdwJ8UjE7"
   },
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 940,
     "status": "ok",
     "timestamp": 1754589056531,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "z2o00oaGUjE7",
    "outputId": "7c553da8-99ad-4fc3-8cab-3b75b9e978b8"
   },
   "outputs": [],
   "source": [
    "# Split dataset into train/validation/test sets\n",
    "print(\"Splitting dataset...\")\n",
    "train_data, val_data, test_data = dataset_utils.split_dataset(\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.20,\n",
    "    test_ratio=0.10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\n Data Split Results:\")\n",
    "print(f\"Training set: {len(train_data['annotations']):,} annotations\")\n",
    "print(f\"Validation set: {len(val_data['annotations']):,} annotations\")\n",
    "print(f\"Test set: {len(test_data['annotations']):,} annotations\")\n",
    "print(f\"Total: {len(train_data['annotations']) + len(val_data['annotations']) + len(test_data['annotations']):,} annotations\")\n",
    "\n",
    "# Calculate percentages\n",
    "total = len(train_data['annotations']) + len(val_data['annotations']) + len(test_data['annotations'])\n",
    "train_pct = len(train_data['annotations']) / total * 100\n",
    "val_pct = len(val_data['annotations']) / total * 100\n",
    "test_pct = len(test_data['annotations']) / total * 100\n",
    "\n",
    "print(f\"\\nPercentages:\")\n",
    "print(f\"Training: {train_pct:.1f}%\")\n",
    "print(f\"Validation: {val_pct:.1f}%\")\n",
    "print(f\"Test: {test_pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567
    },
    "executionInfo": {
     "elapsed": 400,
     "status": "ok",
     "timestamp": 1754589056935,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "0564XPGnUjE8",
    "outputId": "f3eeb622-cb51-44a4-c32c-509497899df1"
   },
   "outputs": [],
   "source": [
    "# Analyze data split quality\n",
    "split_analysis_fig = visualizer.plot_data_split_analysis(\n",
    "    train_data, val_data, test_data,\n",
    "    save_path=f\"{DATA_DIR}/analysis_plots/data_split_analysis.png\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Check category overlap\n",
    "train_cats = set(ann['category_id'] for ann in train_data['annotations'])\n",
    "val_cats = set(ann['category_id'] for ann in val_data['annotations'])\n",
    "test_cats = set(ann['category_id'] for ann in test_data['annotations'])\n",
    "\n",
    "print(f\"\\n Category Overlap Analysis:\")\n",
    "print(f\"Categories in train set: {len(train_cats)}\")\n",
    "print(f\"Categories in val set: {len(val_cats)}\")\n",
    "print(f\"Categories in test set: {len(test_cats)}\")\n",
    "print(f\"Categories in all three sets: {len(train_cats & val_cats & test_cats)}\")\n",
    "print(f\"Categories only in train: {len(train_cats - val_cats - test_cats)}\")\n",
    "print(f\"Categories only in val: {len(val_cats - train_cats - test_cats)}\")\n",
    "print(f\"Categories only in test: {len(test_cats - train_cats - val_cats)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1754589057328,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "zTK22LIqUjE8",
    "outputId": "05650b52-f8e6-48c5-a622-d3bb1034e8b1"
   },
   "outputs": [],
   "source": [
    "# Save split data\n",
    "print(\"Saving split datasets...\")\n",
    "os.makedirs(f\"{DATA_DIR}/annotations\", exist_ok=True)\n",
    "\n",
    "dataset_utils.save_split_data(train_data, val_data, test_data,\n",
    "                             output_dir=f\"{DATA_DIR}/annotations\")\n",
    "\n",
    "print(\"Split datasets saved successfully!\")\n",
    "print(f\"Files saved:\")\n",
    "print(f\"- {DATA_DIR}/annotations/train_annotations.json\")\n",
    "print(f\"- {DATA_DIR}/annotations/val_annotations.json\")\n",
    "print(f\"- {DATA_DIR}/annotations/test_annotations.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wX9L-KTnUjE8"
   },
   "source": [
    "## Detectron2 Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1048,
     "status": "ok",
     "timestamp": 1754589058380,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "tgyPglx3UjE8",
    "outputId": "bb91b2aa-c046-4109-b446-91e15aadb68b"
   },
   "outputs": [],
   "source": [
    "# Convert to Detectron2 format\n",
    "from dataset_utils import create_detectron2_dataset_dict\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "# Create dataset dictionaries for Detectron2\n",
    "print(\"Converting to Detectron2 format...\")\n",
    "\n",
    "train_dataset_dicts = create_detectron2_dataset_dict(train_data, f\"{DATA_DIR}/images\")\n",
    "val_dataset_dicts = create_detectron2_dataset_dict(val_data, f\"{DATA_DIR}/images\")\n",
    "test_dataset_dicts = create_detectron2_dataset_dict(test_data, f\"{DATA_DIR}/images\")\n",
    "\n",
    "print(f\"Detectron2 datasets created:\")\n",
    "print(f\"Train: {len(train_dataset_dicts)} images\")\n",
    "print(f\"Val: {len(val_dataset_dicts)} images\")\n",
    "print(f\"Test: {len(test_dataset_dicts)} images\")\n",
    "\n",
    "# Register datasets with Detectron2\n",
    "def get_hieroglyph_dicts(dataset_dicts):\n",
    "    return dataset_dicts\n",
    "\n",
    "# Clear existing registrations if any\n",
    "for d in [\"hieroglyphs_train\", \"hieroglyphs_val\", \"hieroglyphs_test\"]:\n",
    "    if d in DatasetCatalog:\n",
    "        DatasetCatalog.remove(d)\n",
    "\n",
    "# Register datasets\n",
    "DatasetCatalog.register(\"hieroglyphs_train\", lambda: train_dataset_dicts)\n",
    "DatasetCatalog.register(\"hieroglyphs_val\", lambda: val_dataset_dicts)\n",
    "DatasetCatalog.register(\"hieroglyphs_test\", lambda: test_dataset_dicts)\n",
    "\n",
    "# Set metadata\n",
    "category_names = [dataset_utils.categories[cat_id]['name'] for cat_id in sorted(dataset_utils.categories.keys())]\n",
    "\n",
    "MetadataCatalog.get(\"hieroglyphs_train\").set(thing_classes=category_names)\n",
    "MetadataCatalog.get(\"hieroglyphs_val\").set(thing_classes=category_names)\n",
    "MetadataCatalog.get(\"hieroglyphs_test\").set(thing_classes=category_names)\n",
    "\n",
    "print(\"\\n Datasets registered with Detectron2!\")\n",
    "print(f\"Number of categories: {len(category_names)}\")\n",
    "print(f\"Sample categories: {category_names[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSfjjmuhUjE8"
   },
   "source": [
    "## Sample Data Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 965,
     "output_embedded_package_id": "1V4AlPjONCest300QONUAwOV5Nt6fAAW8"
    },
    "executionInfo": {
     "elapsed": 11881,
     "status": "ok",
     "timestamp": 1754589076368,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "nX_cLdYOUjE8",
    "outputId": "7d038bad-484f-4812-8e2e-99b9e1284a2a"
   },
   "outputs": [],
   "source": [
    "# Verify a sample from each dataset\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "import random\n",
    "\n",
    "# Load image and show sample annotations\n",
    "def visualize_detectron2_sample(dataset_dicts, dataset_name, num_samples=2):\n",
    "    metadata = MetadataCatalog.get(dataset_name)\n",
    "\n",
    "    for i, d in enumerate(random.sample(dataset_dicts, min(num_samples, len(dataset_dicts)))):\n",
    "        img = cv2.imread(d[\"file_name\"])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        visualizer = Visualizer(img, metadata=metadata, scale=0.5)\n",
    "        out = visualizer.draw_dataset_dict(d)\n",
    "\n",
    "        plt.figure(figsize=(20, 12))\n",
    "        plt.imshow(out.get_image())\n",
    "        plt.title(f\"{dataset_name} - Sample {i+1} (Image ID: {d['image_id']})\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Image: {d['file_name']}\")\n",
    "        print(f\"Annotations: {len(d['annotations'])}\")\n",
    "        print(f\"Image size: {d['height']}x{d['width']}\")\n",
    "        print(\"-\"* 50)\n",
    "\n",
    "print(\"Visualizing training samples...\")\n",
    "visualize_detectron2_sample(train_dataset_dicts, \"hieroglyphs_train\", num_samples=1)\n",
    "\n",
    "print(\"\\n Visualizing validation samples...\")\n",
    "visualize_detectron2_sample(val_dataset_dicts, \"hieroglyphs_val\", num_samples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5x-1jeSqUjE8"
   },
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 197,
     "status": "ok",
     "timestamp": 1754589095024,
     "user": {
      "displayName": "Margot Belot",
      "userId": "14937136279205858835"
     },
     "user_tz": -120
    },
    "id": "v7ieMClKUjE8",
    "outputId": "e1170b44-b13b-4b86-bf43-b78a3c2fa02a"
   },
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "summary_report = {\n",
    "    \"dataset_info\": {\n",
    "        \"total_images\": len(dataset_utils.images),\n",
    "        \"total_annotations\": len(dataset_utils.annotations),\n",
    "        \"total_categories\": len(dataset_utils.categories),\n",
    "        \"unique_gardiner_codes\": stats['unique_gardiner_codes'],\n",
    "        \"unique_unicode_codes\": stats['unique_unicode_codes']\n",
    "    },\n",
    "    \"data_split\": {\n",
    "        \"train_annotations\": len(train_data['annotations']),\n",
    "        \"val_annotations\": len(val_data['annotations']),\n",
    "        \"test_annotations\": len(test_data['annotations']),\n",
    "        \"train_categories\": len(train_cats),\n",
    "        \"val_categories\": len(val_cats),\n",
    "        \"test_categories\": len(test_cats)\n",
    "    },\n",
    "    \"class_balance\": {\n",
    "        \"frequent_classes\": len(class_balance['frequent_classes']),\n",
    "        \"common_classes\": len(class_balance['common_classes']),\n",
    "        \"rare_classes\": len(class_balance['rare_classes'])\n",
    "    },\n",
    "    \"validation_issues\": {k: len(v) for k, v in issues.items()},\n",
    "    \"files_created\": [\n",
    "        \"train_annotations.json\",\n",
    "        \"val_annotations.json\",\n",
    "        \"test_annotations.json\",\n",
    "        \"gardiner_unicode_mapping.json\",\n",
    "        \"dataset_overview.png\",\n",
    "        \"class_distribution.png\",\n",
    "        \"sample_annotations.png\",\n",
    "        \"data_split_analysis.png\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save summary report\n",
    "summary_path = f\"{DATA_DIR}/data_preparation_summary.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2)\n",
    "\n",
    "print(\"DATA PREPARATION SUMMARY\")\n",
    "print(\"=\"* 50)\n",
    "print(f\"Dataset loaded and validated\")\n",
    "print(f\"{len(dataset_utils.annotations):,} annotations across {len(dataset_utils.categories)} categories\")\n",
    "print(f\"Data split into train ({len(train_data['annotations']):,}), val ({len(val_data['annotations']):,}), test ({len(test_data['annotations']):,})\")\n",
    "print(f\"Datasets registered with Detectron2\")\n",
    "print(f\"Analysis plots and mappings created\")\n",
    "print(f\"Summary report saved to {summary_path}\")\n",
    "\n",
    "print(\"\\n NEXT STEPS:\")\n",
    "print(\"1. Run notebook 02_data_analysis.ipynb for deeper analysis\")\n",
    "print(\"2. Run notebook 03_model_training.ipynb to train the model\")\n",
    "print(\"3. Run notebook 04_model_evaluation.ipynb to evaluate results\")\n",
    "\n",
    "print(\"\\n IMPORTANT NOTES:\")\n",
    "if class_balance['rare_classes']:\n",
    "    print(f\"- {len(class_balance['rare_classes'])} classes have very few examples (<1% of data)\")\n",
    "    print(\"Consider data augmentation or class weighting during training\")\n",
    "if any(len(issues[k]) > 0 for k in issues):\n",
    "    print(\"- Some validation issues were found. Review them before training\")\n",
    "print(\"- Single image dataset: Consider spatial augmentation during training\")\n",
    "print(\"- High number of categories: Expect longer training times\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
